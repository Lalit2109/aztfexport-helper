# Azure DevOps Pipeline for Terraform Export
# Exports Azure resources using aztfexport and pushes to git repositories

trigger:
  - main
  - master

pr: none

# Pipeline parameters
# subscriptionIds: Comma-separated list of subscription IDs or names to export (empty = all subscriptions)
# Supports both subscription IDs and subscription names (case-insensitive matching)
# Example: "sub-id-1,sub-id-2" or "Production Subscription,Development Subscription" or leave empty for all
parameters:
  - name: subscriptionIds
    displayName: 'Subscription IDs or Names (comma-separated, empty for all)'
    type: string
    default: ''

# Schedule: Run weekly on Friday morning at 8:00 AM UTC
schedules:
- cron: "0 8 * * 5"  # Every Friday at 8:00 AM UTC
  displayName: Weekly Terraform Export (Friday Morning)
  branches:
    include:
    - main
    - master
  always: true  # Run even if there are no code changes

variables:
  - group: 'TerraformExport'  # Variable group with configuration
  - name: configPath
    value: 'config/subscriptions.yaml'
  - name: outputDir
    value: '$(Pipeline.Workspace)/exports'
  - name: gitBranch
    value: 'main'
  - name: pushToRepos
    value: 'true'  # Set to 'false' to disable git push

stages:
- stage: Export
  displayName: 'Terraform Export'
  pool:
    vmImage: 'ubuntu-latest'
  jobs:
  - job: ExportJob
    displayName: 'Export Azure Resources'
    # Matrix strategy: one job per subscription/service connection
    # 
    # WORKFLOW:
    # - Scheduled run (parameter empty): All matrix jobs run â†’ processes all subscriptions
    # - Manual run (parameter set): Only matching subscriptions run
    # - SPN mapping: Subscriptions with dedicated SPNs use them, others use default SPN
    #
    # CONFIGURATION:
    # 1. Default entry: Processes all subscriptions without dedicated SPN mappings (uses default service connection)
    # 2. Add matrix entries ONLY for subscriptions that have dedicated service connections
    # 3. Map subscription IDs to service connections in config/subscriptions.yaml (subscription_service_connections)
    #
    # Each matrix entry should have:
    #   - subscriptionId: The Azure subscription ID (required)
    #   - subscriptionName: The subscription display name (optional but recommended for name-based filtering)
    #   - serviceConnection: The Azure DevOps service connection name for that subscription
    strategy:
      matrix:
        # Default entry - processes all subscriptions without dedicated SPN mappings
        # This job uses the default service connection from variable group
        # Python script will filter subscriptions based on parameter and exclusion config
        default:
          subscriptionId: ''  # Empty = process all subscriptions (filtered by Python)
          subscriptionName: ''  # Optional: subscription display name
          serviceConnection: '$(azureServiceConnection)'
        
        # Add matrix entries ONLY for subscriptions with dedicated service connections
        # These subscriptions will use their own SPN instead of the default one
        # Example:
        # subId1:
        #   subscriptionId: '12345678-1234-1234-1234-123456789012'
        #   subscriptionName: 'Production Subscription'  # Optional but recommended
        #   serviceConnection: 'sc-subscription-1'
        # 
        # subId2:
        #   subscriptionId: '87654321-4321-4321-4321-210987654321'
        #   subscriptionName: 'Development Subscription'  # Optional but recommended
        #   serviceConnection: 'sc-subscription-2'
    variables:
      subscriptionId: $[ variables['subscriptionId'] ]
      subscriptionName: $[ coalesce(variables['subscriptionName'], '') ]
      serviceConnection: $[ coalesce(variables['serviceConnection'], variables['azureServiceConnection']) ]
    steps:
    # Step 0: Check if this subscription should be processed
    - script: |
        SHOULD_RUN="true"
        THIS_SUB_ID="$(subscriptionId)"
        THIS_SUB_NAME="$(subscriptionName)"
        
        # If subscriptionId is empty (default matrix entry), always run (Python will filter)
        if [ -z "$THIS_SUB_ID" ]; then
          echo "Default matrix entry - will process subscriptions based on parameter"
          echo "##vso[task.setvariable variable=ShouldRunJob]true"
          exit 0
        fi
        
        # If parameter is set, check if this subscription (by ID or name) is in the list
        if [ -n "${{ parameters.subscriptionIds }}" ]; then
          SUB_IDS="${{ parameters.subscriptionIds }}"
          MATCHED="false"
          
          # Check if subscription ID is in the comma-separated list
          if echo "$SUB_IDS" | grep -qE "(^|,)$THIS_SUB_ID(,|$)"; then
            echo "Subscription ID $THIS_SUB_ID is in the selected list"
            MATCHED="true"
          # Check if subscription name is in the list (if name is provided)
          elif [ -n "$THIS_SUB_NAME" ] && echo "$SUB_IDS" | grep -qiE "(^|,)$THIS_SUB_NAME(,|$)"; then
            echo "Subscription name '$THIS_SUB_NAME' is in the selected list"
            MATCHED="true"
          fi
          
          if [ "$MATCHED" = "false" ]; then
            echo "Subscription (ID: $THIS_SUB_ID, Name: $THIS_SUB_NAME) is not in the selected list: $SUB_IDS"
            echo "Skipping this job."
            SHOULD_RUN="false"
          fi
        fi
        # If parameter is empty, process all (scheduled run)
        
        echo "##vso[task.setvariable variable=ShouldRunJob]$SHOULD_RUN"
        if [ "$SHOULD_RUN" = "false" ]; then
          echo "Job will be skipped - subscription not selected"
        else
          if [ -n "$THIS_SUB_NAME" ]; then
            echo "Processing subscription: $THIS_SUB_NAME (ID: $THIS_SUB_ID)"
          else
            echo "Processing subscription: $THIS_SUB_ID"
          fi
        fi
      displayName: 'Check Subscription Selection'
      continueOnError: false
    
    # Step 1: Install prerequisites
    - task: GoTool@0
      displayName: 'Install Go (for aztfexport)'
      condition: eq(variables['ShouldRunJob'], 'true')
      inputs:
        version: '1.21.x'

    - script: |
        echo "Installing aztfexport..."
        go install github.com/Azure/aztfexport@latest
        export PATH=$PATH:$(go env GOPATH)/bin
        aztfexport --version
        echo "##vso[task.prependpath]$(go env GOPATH)/bin"
      displayName: 'Install aztfexport'
      condition: eq(variables['ShouldRunJob'], 'true')

    - task: UsePythonVersion@0
      displayName: 'Setup Python'
      condition: eq(variables['ShouldRunJob'], 'true')
      inputs:
        versionSpec: '3.10'
        addToPath: true

    - script: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
      displayName: 'Install Python dependencies'
      condition: eq(variables['ShouldRunJob'], 'true')

    - task: TerraformInstaller@0
      displayName: 'Install Terraform'
      condition: eq(variables['ShouldRunJob'], 'true')
      inputs:
        terraformVersion: 'latest'

    # Step 2: Clean previous exports
    - script: |
        echo "Cleaning output directory: $(outputDir)"
        if [ -d "$(outputDir)" ]; then
          rm -rf "$(outputDir)"
        fi
        mkdir -p "$(outputDir)"
        echo "Output directory is now clean:"
        ls -la "$(outputDir)" || echo "Directory is empty"
      displayName: 'Clean export/output directory'
      condition: eq(variables['ShouldRunJob'], 'true')

    # Step 3: Configure Azure authentication and run export
    - task: AzureCLI@2
      displayName: 'Azure Login and Export'
      condition: eq(variables['ShouldRunJob'], 'true')
      inputs:
        azureSubscription: '$(serviceConnection)'  # Service connection for this subscription (from matrix)
        scriptType: 'bash'
        scriptLocation: 'inlineScript'
        inlineScript: |
          echo "Authenticating with Azure..."
          echo "Using service connection: $(serviceConnection)"
          az account show
          echo "##vso[task.setvariable variable=AZURE_SUBSCRIPTION_ID]$(az account show --query id -o tsv)"
          echo "Logged in to subscription: $(az account show --query name -o tsv)"
          
          # Verify Terraform is available
          echo "Checking Terraform installation..."
          terraform --version || echo "Warning: Terraform not found in PATH"
          which terraform || echo "Warning: terraform command not found"
          
          # Configure Git
          git config --global user.name "Azure Pipelines"
          git config --global user.email "azure-pipelines@devops.com"
          
          # Set up environment variables
          export PUSH_TO_REPOS=$(pushToRepos)
          export GIT_BRANCH=$(gitBranch)
          export OUTPUT_DIR=$(outputDir)
          export LOG_LEVEL=INFO
          export CONFIG_PATH=$(configPath)
          export AZURE_DEVOPS_PAT=$(System.AccessToken)
          export SYSTEM_ACCESS_TOKEN=$(System.AccessToken)
          # Pass subscription ID to Python script for filtering
          export SUBSCRIPTION_IDS="$(subscriptionId)"
          # Log Analytics configuration (set in variable group or pipeline variables)
          export LOG_ANALYTICS_WORKSPACE_ID=$(LOG_ANALYTICS_WORKSPACE_ID)
          export LOG_ANALYTICS_SHARED_KEY=$(LOG_ANALYTICS_SHARED_KEY)
          
          # Run export
          echo "Starting Terraform export for subscription: $(subscriptionId)"
          python src/main.py
      env:
        SYSTEM_ACCESS_TOKEN: $(System.AccessToken)
        LOG_ANALYTICS_WORKSPACE_ID: $(LOG_ANALYTICS_WORKSPACE_ID)
        LOG_ANALYTICS_SHARED_KEY: $(LOG_ANALYTICS_SHARED_KEY)

    # Step 6: Publish export results as artifacts
    - script: |
        # Ensure output directory exists
        if [ ! -d "$(outputDir)" ]; then
          echo "Creating output directory: $(outputDir)"
          mkdir -p "$(outputDir)"
        fi
        # Check if directory has content
        if [ -z "$(ls -A $(outputDir) 2>/dev/null)" ]; then
          echo "Warning: Output directory is empty, creating placeholder"
          echo "No exports completed" > "$(outputDir)/README.txt"
        fi
        echo "Output directory contents:"
        ls -la "$(outputDir)" || echo "Directory is empty"
      displayName: 'Prepare Output Directory'
      condition: eq(variables['ShouldRunJob'], 'true')
      continueOnError: true

    - task: PublishBuildArtifacts@1
      displayName: 'Publish Export Results'
      condition: and(eq(variables['ShouldRunJob'], 'true'), always())
      inputs:
        pathToPublish: '$(outputDir)'
        artifactName: 'terraform-exports'
        publishLocation: 'Container'
      condition: always()

    # Step 7: Generate summary report
    - script: |
        if [ -f "$(outputDir)/export_results.json" ]; then
          echo "##[section]Export Summary"
          echo '```json'
          cat "$(outputDir)/export_results.json" | python -m json.tool
          echo '```'
        else
          echo "Export results file not found"
        fi
      displayName: 'Generate Summary Report'
      condition: eq(variables['ShouldRunJob'], 'true')
      continueOnError: true

- stage: SendEmailReport
  displayName: 'Send Backup Email Report'
  dependsOn: Export
  condition: always()  # Run even if export stage fails
  pool:
    vmImage: 'windows-latest'  # Windows needed for PowerShell
  jobs:
  - job: EmailReportJob
    displayName: 'Send Email Report'
    steps:
    - task: AzurePowerShell@5
      displayName: 'Send Terraform Backup Email Report'
      inputs:
        azureSubscription: '$(azureServiceConnection)'
        ScriptType: 'FilePath'
        ScriptPath: 'azure-pipelines/backup-monitor/send-backup-report.ps1'
        ScriptArguments: >
          -LogAnalyticsWorkspaceId "$(LOG_ANALYTICS_WORKSPACE_ID)"
          -LogAnalyticsSharedKey "$(LOG_ANALYTICS_SHARED_KEY)"
          -SendGridApiKey "$(SendGridApiKey)"
          -SendGridFrom "$(SendGridFrom)"
          -SendGridTo "$(SendGridTo)"
          -EnvName "$(Environment)"
          -LookbackHours 24
        azurePowerShellVersion: 'LatestVersion'
      continueOnError: true  # Don't fail pipeline if email fails

